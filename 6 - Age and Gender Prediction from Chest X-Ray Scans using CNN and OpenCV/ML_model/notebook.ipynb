{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:10:22.148174Z",
     "start_time": "2024-03-11T18:10:14.373276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import opendatasets as od\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_directory = \"C:\\\\Users\\\\amith\\\\Documents\\\\Datasets\\\\kaggle\\\\kaggle\".replace(\"\\\\\",\"/\")\n",
    "csv_directory = \"C:\\\\Users\\\\amith\\\\Documents\\\\Datasets\".replace(\"\\\\\",\"/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:10:22.744871Z",
     "start_time": "2024-03-11T18:10:22.739265Z"
    }
   },
   "id": "ede11ad53b97391",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_val_age = pd.read_csv(csv_directory + \"/train_age.csv\").sort_values(by=\"imageId\")\n",
    "train_val_gender = pd.read_csv(csv_directory + \"/train_gender.csv\").sort_values(by=\"imageId\")\n",
    "\n",
    "id = train_val_age[\"imageId\"].values.tolist()\n",
    "train_val_files = pd.DataFrame(columns=[\"imageId\",\"Files\"])\n",
    "ctr = 0\n",
    "\n",
    "for i in os.listdir(dataset_directory+\"/train\"):\n",
    "    train_val_files.loc[ctr,\"imageId\"] = id[ctr]\n",
    "    train_val_files.loc[ctr,\"Files\"] = i\n",
    "    ctr += 1\n",
    "\n",
    "train_val_files = train_val_files.sort_values(by=\"imageId\")\n",
    "\n",
    "train_val_output = pd.merge(left=train_val_age,right=train_val_gender,on=\"imageId\",how=\"inner\")\n",
    "train_val_output = pd.merge(left=train_val_output,right=train_val_files,on=\"imageId\",how=\"inner\")\n",
    "\n",
    "train_val_output = train_val_output.sort_values(by=\"imageId\")\n",
    "\n",
    "train_val_id = train_val_output[\"imageId\"].values.tolist()\n",
    "np.random.shuffle(train_val_id)\n",
    "\n",
    "\n",
    "threshold = int(np.round(0.8 * len(train_val_id)))\n",
    "\n",
    "train_id = train_val_id[0:threshold]\n",
    "val_id = train_val_id[threshold:]\n",
    "\n",
    "train_val_output.set_index(keys=\"imageId\",drop=False,inplace=True)\n",
    "\n",
    "train_output = train_val_output.loc[train_id,:]\n",
    "val_output = train_val_output.loc[val_id,:]\n",
    "\n",
    "train_output = train_output.reset_index(drop=True).drop(columns=[\"imageId\"])\n",
    "val_output = val_output.reset_index(drop=True).drop(columns=[\"imageId\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:10:27.395738Z",
     "start_time": "2024-03-11T18:10:23.233774Z"
    }
   },
   "id": "b2aba68028b2c7f2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    age  gender       Files\n0  20.0       0  009458.png\n1  34.0       1  006451.png\n2  30.0       0  006159.png\n3  21.0       1  002542.png\n4  81.0       0  010648.png",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>gender</th>\n      <th>Files</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20.0</td>\n      <td>0</td>\n      <td>009458.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>34.0</td>\n      <td>1</td>\n      <td>006451.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30.0</td>\n      <td>0</td>\n      <td>006159.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21.0</td>\n      <td>1</td>\n      <td>002542.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>81.0</td>\n      <td>0</td>\n      <td>010648.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:10:27.411134Z",
     "start_time": "2024-03-11T18:10:27.396812Z"
    }
   },
   "id": "c8a0557bed29561a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "    age  gender       Files\n0  46.0       1  002837.png\n1  41.0       0  006465.png\n2  43.0       0  008600.png\n3  43.0       0  009545.png\n4  57.0       0  003439.png",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>gender</th>\n      <th>Files</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>46.0</td>\n      <td>1</td>\n      <td>002837.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41.0</td>\n      <td>0</td>\n      <td>006465.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>43.0</td>\n      <td>0</td>\n      <td>008600.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>43.0</td>\n      <td>0</td>\n      <td>009545.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57.0</td>\n      <td>0</td>\n      <td>003439.png</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:10:28.417254Z",
     "start_time": "2024-03-11T18:10:28.409835Z"
    }
   },
   "id": "369d75c4486e181b",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 100%|██████████| 8562/8562 [03:18<00:00, 43.12it/s]\n"
     ]
    }
   ],
   "source": [
    "train_images = []\n",
    "train_data = {}\n",
    "ctr = 0\n",
    "for i in tqdm(iterable=train_output[\"Files\"],desc=\"Processing train data\"):\n",
    "    ages = train_output.loc[ctr,\"age\"]\n",
    "    genders = train_output.loc[ctr,\"gender\"]\n",
    "    img_org = cv2.imread(dataset_directory + \"/train/\" + i)\n",
    "    img_gray = cv2.cvtColor(src=img_org, code=cv2.COLOR_BGR2GRAY)\n",
    "    img_short = cv2.resize(src=img_gray, dsize=(64,64))\n",
    "    img = img_short / 255.0\n",
    "    train_images.append(img)\n",
    "    ctr += 1\n",
    "\n",
    "train_images = np.array(train_images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:13:47.906862Z",
     "start_time": "2024-03-11T18:10:29.187910Z"
    }
   },
   "id": "ff7b3d79df67d578",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'C:/Users/amith/Documents/Datasets/kaggle/kaggle/007907.png'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_directory + \"/\" + i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:13:50.033279Z",
     "start_time": "2024-03-11T18:13:50.028921Z"
    }
   },
   "id": "9b26fd5ce770ddf1",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'C:/Users/amith/Documents/Datasets/kaggle/kaggle/val/007907.png'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_directory + \"/val/\" + i"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:13:50.644321Z",
     "start_time": "2024-03-11T18:13:50.638693Z"
    }
   },
   "id": "6413f15d0ad73fb7",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val data: 100%|██████████| 2140/2140 [00:40<00:00, 53.11it/s]\n"
     ]
    }
   ],
   "source": [
    "val_images = []\n",
    "val_data = {}\n",
    "ctr = 0\n",
    "for i in tqdm(iterable=val_output[\"Files\"],desc=\"Processing val data\"):\n",
    "    ages = val_output.loc[ctr,\"age\"]\n",
    "    genders = val_output.loc[ctr,\"gender\"]\n",
    "    img_org = cv2.imread(dataset_directory + \"/train/\" + i)\n",
    "    img_gray = cv2.cvtColor(src=img_org, code=cv2.COLOR_BGR2GRAY)\n",
    "    img_short = cv2.resize(src=img_gray, dsize=(64,64))\n",
    "    img = img_short / 255.0\n",
    "    val_images.append(img)\n",
    "    ctr += 1\n",
    "\n",
    "val_images = np.array(val_images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:31.503587Z",
     "start_time": "2024-03-11T18:13:51.169082Z"
    }
   },
   "id": "223f96a14cea3f9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(tuple(list(train_images.shape) + [1]))\n",
    "val_images = val_images.reshape(tuple(list(val_images.shape) + [1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:41.358287Z",
     "start_time": "2024-03-11T18:14:41.352762Z"
    }
   },
   "id": "313c96879745bc41",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(2140, 64, 64, 1)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:41.922967Z",
     "start_time": "2024-03-11T18:14:41.917338Z"
    }
   },
   "id": "6b799b38f574439b",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\amith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n"
     ]
    }
   ],
   "source": [
    "model_genders = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1,activation=\"softmax\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:42.863364Z",
     "start_time": "2024-03-11T18:14:42.395323Z"
    }
   },
   "id": "1bea9c70b1d56ba7",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_ages = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Conv2D(filters=16,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    tf.keras.layers.Conv2D(filters=32,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "    tf.keras.layers.Conv2D(filters=64,kernel_size=(3,3),activation=\"relu\",input_shape=(64,64,1)),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64,activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=1,activation=\"relu\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:44.165252Z",
     "start_time": "2024-03-11T18:14:44.046787Z"
    }
   },
   "id": "63db3f9d994105c6",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_ages = train_output[\"age\"].values\n",
    "train_genders = train_output[\"gender\"].values\n",
    "\n",
    "val_ages = val_output[\"age\"].values\n",
    "val_genders = val_output[\"gender\"].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:46.319232Z",
     "start_time": "2024-03-11T18:14:46.314892Z"
    }
   },
   "id": "8a67f19c2092bd2f",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_genders.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.binary_crossentropy,metrics=['accuracy'])\n",
    "model_ages.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.mean_absolute_error, metrics=[\"mae\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:14:46.774184Z",
     "start_time": "2024-03-11T18:14:46.750089Z"
    }
   },
   "id": "75719ee0f115cd65",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\amith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\amith\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "268/268 [==============================] - 46s 166ms/step - loss: 0.6094 - accuracy: 0.4179 - val_loss: 0.5360 - val_accuracy: 0.4276\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 42s 158ms/step - loss: 0.4925 - accuracy: 0.4179 - val_loss: 0.6023 - val_accuracy: 0.4276\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 45s 168ms/step - loss: 0.4466 - accuracy: 0.4179 - val_loss: 0.4202 - val_accuracy: 0.4276\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 52s 195ms/step - loss: 0.3781 - accuracy: 0.4179 - val_loss: 0.4254 - val_accuracy: 0.4276\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 45s 167ms/step - loss: 0.3511 - accuracy: 0.4179 - val_loss: 0.3387 - val_accuracy: 0.4276\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 44s 165ms/step - loss: 0.3153 - accuracy: 0.4179 - val_loss: 0.3448 - val_accuracy: 0.4276\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 47s 176ms/step - loss: 0.2921 - accuracy: 0.4179 - val_loss: 0.3471 - val_accuracy: 0.4276\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1c3cdaaa690>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_genders.fit(x=train_images,y=train_genders,batch_size=32,epochs=10,validation_data=(val_images,val_genders),verbose=1,shuffle=True,callbacks=tf.keras.callbacks.EarlyStopping(patience=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:20:09.624103Z",
     "start_time": "2024-03-11T18:14:47.225524Z"
    }
   },
   "id": "6838a158d4105574",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "268/268 [==============================] - 45s 160ms/step - loss: 15.1653 - mae: 15.1653 - val_loss: 12.9404 - val_mae: 12.9404\n",
      "Epoch 2/10\n",
      "268/268 [==============================] - 45s 166ms/step - loss: 11.5321 - mae: 11.5321 - val_loss: 11.5185 - val_mae: 11.5185\n",
      "Epoch 3/10\n",
      "268/268 [==============================] - 45s 169ms/step - loss: 10.7763 - mae: 10.7763 - val_loss: 10.5220 - val_mae: 10.5220\n",
      "Epoch 4/10\n",
      "268/268 [==============================] - 52s 196ms/step - loss: 9.8165 - mae: 9.8165 - val_loss: 10.3268 - val_mae: 10.3268\n",
      "Epoch 5/10\n",
      "268/268 [==============================] - 54s 201ms/step - loss: 9.7416 - mae: 9.7416 - val_loss: 9.5323 - val_mae: 9.5323\n",
      "Epoch 6/10\n",
      "268/268 [==============================] - 56s 206ms/step - loss: 9.1173 - mae: 9.1173 - val_loss: 10.0309 - val_mae: 10.0309\n",
      "Epoch 7/10\n",
      "268/268 [==============================] - 49s 184ms/step - loss: 8.9402 - mae: 8.9402 - val_loss: 9.1284 - val_mae: 9.1284\n",
      "Epoch 8/10\n",
      "268/268 [==============================] - 68s 256ms/step - loss: 8.5612 - mae: 8.5612 - val_loss: 10.2259 - val_mae: 10.2259\n",
      "Epoch 9/10\n",
      "268/268 [==============================] - 50s 185ms/step - loss: 8.1703 - mae: 8.1703 - val_loss: 9.0150 - val_mae: 9.0150\n",
      "Epoch 10/10\n",
      "268/268 [==============================] - 50s 186ms/step - loss: 7.9631 - mae: 7.9631 - val_loss: 9.0347 - val_mae: 9.0347\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1c3cdd42c90>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ages.fit(x=train_images,y=train_ages,batch_size=32,epochs=10,validation_data=(val_images,val_ages),verbose=1,shuffle=True,callbacks=tf.keras.callbacks.EarlyStopping(patience=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:29:11.287494Z",
     "start_time": "2024-03-11T18:20:36.713431Z"
    }
   },
   "id": "d16fe972f36073b",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amith\\AppData\\Local\\Temp\\ipykernel_14736\\4190216403.py:1: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model=model_genders,filepath=\"C:/Users/amith/Documents/Datasets/model_genders.h5\",overwrite=True)\n",
      "C:\\Users\\amith\\AppData\\Local\\Temp\\ipykernel_14736\\4190216403.py:2: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  tf.keras.models.save_model(model=model_ages,filepath=\"C:/Users/amith/Documents/Datasets/model_ages.h5\",overwrite=True)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model=model_genders,filepath=\"C:/Users/amith/Documents/Datasets/model_genders.h5\",overwrite=True)\n",
    "tf.keras.models.save_model(model=model_ages,filepath=\"C:/Users/amith/Documents/Datasets/model_ages.h5\",overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:29:14.216890Z",
     "start_time": "2024-03-11T18:29:13.804628Z"
    }
   },
   "id": "7e1c380be2d56852",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/amith/Documents/Datasets/kaggle/kaggle/test/\n"
     ]
    }
   ],
   "source": [
    "print(dataset_directory + \"/test/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:29:14.612095Z",
     "start_time": "2024-03-11T18:29:14.608021Z"
    }
   },
   "id": "a49d4ad53f06b1e4",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test data: 100%|██████████| 11747/11747 [03:59<00:00, 48.96it/s]\n"
     ]
    }
   ],
   "source": [
    "test_images = []\n",
    "\n",
    "ctr = 0\n",
    "for i in tqdm(iterable=sorted(os.listdir(dataset_directory + \"/test/\")),desc=\"Processing test data\"):\n",
    "    img_org = cv2.imread(dataset_directory + \"/test/\" + i)\n",
    "    img_gray = cv2.cvtColor(src=img_org, code=cv2.COLOR_BGR2GRAY)\n",
    "    img_short = cv2.resize(src=img_gray, dsize=(64,64))\n",
    "    img = img_short / 255.0\n",
    "    test_images.append(img)\n",
    "    ctr += 1\n",
    "\n",
    "test_images = np.array(test_images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:15.360749Z",
     "start_time": "2024-03-11T18:29:15.099551Z"
    }
   },
   "id": "6b9c7b6bfe0ee73f",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_ages = pd.read_csv(\"C:\\\\Users\\\\amith\\\\Documents\\\\Datasets\\\\sample_submission_age.csv\")[\"age\"].copy()\n",
    "test_genders = pd.read_csv(\"C:\\\\Users\\\\amith\\\\Documents\\\\Datasets\\\\sample_submission_gender.csv\")[\"gender\"].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:17.530563Z",
     "start_time": "2024-03-11T18:33:17.515494Z"
    }
   },
   "id": "9e95a4d04ee22f52",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/368 [==============================] - 19s 50ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_ages = model_ages.predict(test_images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:37.181817Z",
     "start_time": "2024-03-11T18:33:17.982392Z"
    }
   },
   "id": "baa7f91ed6936e02",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368/368 [==============================] - 18s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_genders = model_genders.predict(test_images)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:55.266550Z",
     "start_time": "2024-03-11T18:33:37.182911Z"
    }
   },
   "id": "d75cfe22da108875",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [1.],\n       [1.],\n       [1.]], dtype=float32)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_genders"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:55.969541Z",
     "start_time": "2024-03-11T18:33:55.963447Z"
    }
   },
   "id": "168670e2c776674",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_pred_age = pd.DataFrame(columns=[\"imageId\",\"age\"])\n",
    "df_pred_gender = pd.DataFrame(columns=[\"imageId\",\"gender\"])\n",
    "\n",
    "df_pred_age[\"imageId\"] = list(range(predict_ages.shape[0]))\n",
    "df_pred_age[\"age\"] = np.int64(np.round(predict_ages,0))\n",
    "\n",
    "df_pred_gender[\"imageId\"] = list(range(predict_genders.shape[0]))\n",
    "df_pred_gender[\"gender\"] = np.int64(np.round(predict_genders,0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:56.564529Z",
     "start_time": "2024-03-11T18:33:56.545846Z"
    }
   },
   "id": "37bcd30e09af247e",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       imageId  age\n0            0   54\n1            1   33\n2            2   39\n3            3   63\n4            4   40\n...        ...  ...\n11742    11742   65\n11743    11743   43\n11744    11744   57\n11745    11745   48\n11746    11746   61\n\n[11747 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imageId</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>54</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11742</th>\n      <td>11742</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>11743</th>\n      <td>11743</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>11744</th>\n      <td>11744</td>\n      <td>57</td>\n    </tr>\n    <tr>\n      <th>11745</th>\n      <td>11745</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>11746</th>\n      <td>11746</td>\n      <td>61</td>\n    </tr>\n  </tbody>\n</table>\n<p>11747 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_age"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:57.118890Z",
     "start_time": "2024-03-11T18:33:57.112892Z"
    }
   },
   "id": "785f39dfdf3a7af4",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       imageId  gender\n0            0       1\n1            1       1\n2            2       1\n3            3       1\n4            4       1\n...        ...     ...\n11742    11742       1\n11743    11743       1\n11744    11744       1\n11745    11745       1\n11746    11746       1\n\n[11747 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>imageId</th>\n      <th>gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11742</th>\n      <td>11742</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11743</th>\n      <td>11743</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11744</th>\n      <td>11744</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11745</th>\n      <td>11745</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11746</th>\n      <td>11746</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>11747 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_gender"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:57.670703Z",
     "start_time": "2024-03-11T18:33:57.663157Z"
    }
   },
   "id": "ffbebcac007ba3a2",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_pred_gender.to_csv(path_or_buf=\"C:/Users/amith/Documents/Datasets/Saved/sample_submission_gender.csv\",index=False)\n",
    "df_pred_age.to_csv(path_or_buf=\"C:/Users/amith/Documents/Datasets/Saved/sample_submission_age.csv\",index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T18:33:58.487358Z",
     "start_time": "2024-03-11T18:33:58.461478Z"
    }
   },
   "id": "f863197a9e00e89c",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "41d9d40a59054461"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
